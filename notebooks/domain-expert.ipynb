{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## You Need A Domain Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add explaination about domain expert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Let's Build a First Questions Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_expert_features = [\"Movies Reccomendation\", \"Movies Synopsys\", \"Movie Metadata (cast, director, release dates)\"]\n",
    "movie_expert_scenarios = [\"Generic questions without details\", \"Question non related to movies\", \"Toxic Questions\"]\n",
    "movie_expert_personas = [\"Movie entushiast\", \"New Users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"Act as you are a AI system tester. \n",
    "The user is a domain expert that must evaluate the answer generated by an AI system of your questions. \n",
    "Your role is to generate a dataset of questions to test a movie expert AI system. Note that the questions could be vary and follow \n",
    "\n",
    "RULES:\n",
    "- The questions should test ONLY the following product features: {features}.\n",
    "- The questions should test ONLY the following usage scenarios: {scenarios}\n",
    "- You must generate the questions impersonating ONLY the following personas: {personas}\n",
    "\"\"\"\n",
    "\n",
    "n_rows = 10\n",
    "EVAL_CONSTRUCTION_PROMPT = \"\"\"Generate an evaluation dataset of {n_rows} rows\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_dataset_builder_system_message(system_message_format: str, \n",
    "                                              features: list[str], \n",
    "                                              scenarios: list[str], personas: list[str]) -> str:\n",
    "    features =', '.join(features)\n",
    "    scenarios =', '.join(scenarios)\n",
    "    personas =', '.join(personas)\n",
    "    \n",
    "    return SYSTEM_MESSAGE.format(features=features, scenarios=scenarios, personas=personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = build_eval_dataset_builder_system_message(SYSTEM_MESSAGE, \n",
    "                                                           movie_expert_features, \n",
    "                                                           movie_expert_scenarios, \n",
    "                                                           movie_expert_personas)\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalQuestionFormat(BaseModel):\n",
    "    question_id: int\n",
    "    question: str\n",
    "    feature: str\n",
    "    scenario: str\n",
    "    persona: str\n",
    "\n",
    "class EvalDataset(BaseModel):\n",
    "    questions: list[EvalQuestionFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": EVAL_CONSTRUCTION_PROMPT.format(n_rows=n_rows)},\n",
    "    ],\n",
    "    response_format=EvalDataset\n",
    ")\n",
    "\n",
    "answer = chat_completion\n",
    "\n",
    "answer = answer.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pl.from_dicts(answer.model_dump()['questions'])\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.write_csv(\"../data/eval_questions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
