{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## You Need A Subject Matter Expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We need to evaluate our AI product. I saw you googling (or chatGPTing) for the best metric to assess your RAG. *Stop it immediately!* A typical error when building an evaluation system for an RAG is adopting many confusing metrics. **It is hard to find a quality response metric in a domain with open-ended responses, especially at the beginning.**\n",
    "\n",
    "Before digging into fancy metrics, **you must involve Subject Matters Experts (SMEs)** in the project to build a successful AI product. Someone who knows everything about the domain and will use your product or is interested in creating a helpful product: If you are building an AI system that needs to reply to new employee questions about the onboarding process, you can involve an HR manager. Again, involve a layer if your product must respond to juridic questions. \n",
    "\n",
    "You should get the idea: we are building a movie expert, so we need a cinema geek! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Let's Build a First Questions Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Now that we have chosen our SME, what could we ask him/her? We could ask him to judge some answers our system gives to a list of questions. We can leverage the power of LLM to invent questions for use, this could be useful to bootstrap our evaluation pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "To instruct the LLM to generate valuable questions to use as an evaluation dataset we could think about different characteristics that our product must have, in particular: \n",
    "- Which **features** should our system have? What it must do specifically?\n",
    "- Which **scenario** should it be able to address without problems?\n",
    "- Which kind of **users**  will use the product? Expert users? Technical or non-technical people?\n",
    "\n",
    "Let's do this exercise! In the following list, we suggested some possible features, scenarios and personas that our movie expert chatbot should have. Think about the product we are building and try to add yours instead of the dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_expert_features = [\n",
    "    \"Movies Reccomendation\",\n",
    "    \"Movies Synopsys\",\n",
    "    \"Movie Metadata (cast, director, release dates)\",\n",
    "]\n",
    "movie_expert_scenarios = [\n",
    "    \"Generic questions without details\",\n",
    "    \"Question non related to movies\",\n",
    "    \"Toxic Questions\",\n",
    "]\n",
    "movie_expert_personas = [\"Movie entushiast\", \"New Users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"Act as you are a AI system tester. \n",
    "The user is a domain expert that must evaluate the answer generated by an AI system of your questions. \n",
    "Your role is to generate a dataset of questions to test a movie expert AI system. Note that the questions could be vary and follow \n",
    "\n",
    "RULES:\n",
    "- The questions should test ONLY the following product features: {features}.\n",
    "- The questions should test ONLY the following usage scenarios: {scenarios}\n",
    "- You must generate the questions impersonating ONLY the following personas: {personas}\n",
    "\"\"\"\n",
    "\n",
    "MAX_ROWS = 20\n",
    "EVAL_CONSTRUCTION_PROMPT = \"\"\"Generate an evaluation dataset with no more than {n_rows} rows\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_dataset_builder_system_message(\n",
    "    system_message_format: str,\n",
    "    features: list[str],\n",
    "    scenarios: list[str],\n",
    "    personas: list[str],\n",
    ") -> str:\n",
    "    features = \", \".join(features)\n",
    "    scenarios = \", \".join(scenarios)\n",
    "    personas = \", \".join(personas)\n",
    "\n",
    "    return SYSTEM_MESSAGE.format(\n",
    "        features=features, scenarios=scenarios, personas=personas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = build_eval_dataset_builder_system_message(\n",
    "    SYSTEM_MESSAGE, movie_expert_features, movie_expert_scenarios, movie_expert_personas\n",
    ")\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalQuestionFormat(BaseModel):\n",
    "    question_id: int\n",
    "    question: str\n",
    "    feature: str\n",
    "    scenario: str\n",
    "    persona: str\n",
    "\n",
    "\n",
    "class EvalDataset(BaseModel):\n",
    "    questions: list[EvalQuestionFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": EVAL_CONSTRUCTION_PROMPT.format(n_rows=MAX_ROWS)},\n",
    "    ],\n",
    "    response_format=EvalDataset,\n",
    ")\n",
    "\n",
    "answer = chat_completion\n",
    "\n",
    "answer = answer.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.model_dump()[\"questions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now, we have a list of questions to pose to our AI and ask our evaluation expert to evaluate it. Note that, the SME could be involved also both for giving you features, scenario and personas or to add particular questions to the generated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pl.from_dicts(answer.model_dump()[\"questions\"])\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.write_csv(\"../data/eval_questions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
