{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## You Need A Subject Matter Expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We need to evaluate our AI product. I saw you googling (or chatGPTing) for the best metric to assess your RAG. *Stop it immediately!* A typical error when building an evaluation system for an RAG is adopting many confusing metrics. **It is hard to find a quality response metric in a domain with open-ended responses, especially at the beginning.**\n",
    "\n",
    "Before digging into fancy metrics, **you must involve Subject Matters Experts (SMEs)** in the project to build a successful AI product. Someone who knows everything about the domain and will use your product or is interested in creating a helpful product: If you are building an AI system that needs to reply to new employee questions about the onboarding process, you can involve an HR manager. Again, involve a lawyer if your product must respond to legal questions.\n",
    "\n",
    "It's even better if you can identify the *principal* subject matter expert. As Hamel says, the principal SME is someone \"whose judgment is crucial for the success of your AI product. These are the people with deep domain expertise or represent your target users.\" There are a couple of reasons why you should find them: they set the standard and, since they are at most two people, their judgment will be consistent. Also, their involvement might make them feel owners of the projects.\n",
    "\n",
    "By now, you should get the idea: since we are building a movie expert, we need a cinema geek! Yes, it might be hard to find one in the room. But perhaps you can turn to the person sitting next to you, and treat them as your expert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Let's Build a First Questions Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Now that we have identified our SME, we need to ask them to evaluate the interactions with the chatbot. But how can we, if we have no data?\n",
    "\n",
    "Here is another situation where LLMs can come in help: we can simply instruct them to generate the prompts for us. Two caveats:\n",
    "\n",
    "1. Use the smartest model available. Evals are expensive, but it's far more expensive to make changes to your system without running them!\n",
    "2. Perhaps synthetic data might not cover every nuance with real-world interactions. It's not a real problem: we are trying to get from zero to one, and we can always use (part of) the real-world data we collect from usage to build more eval datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "To instruct the LLM to generate valuable questions to use as an evaluation dataset we could think about different characteristics that our product must have, in particular: \n",
    "\n",
    "- Which **features** should our system have? What it must do specifically?\n",
    "- Which **scenario** should it be able to address without problems?\n",
    "- Which kind of **users**  will use the product? Expert users? Technical or non-technical people?\n",
    "\n",
    "Let's do this exercise! In the following list, we suggested some possible features, scenarios and personas that our movie expert chatbot should have. Think about the product we are building and try to add yours instead of the dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_expert_features = [\n",
    "    \"Movies Recommendation\",\n",
    "    \"Movies Synopsys\",\n",
    "    \"Movie Metadata (cast, director, release dates)\",\n",
    "]\n",
    "movie_expert_scenarios = [\n",
    "    \"Generic questions without details\",\n",
    "    \"Question non related to movies\",\n",
    "    \"Toxic Questions\",\n",
    "]\n",
    "movie_expert_personas = [\"Movie entushiast\", \"New Users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"Act as you are a AI system tester. \n",
    "The user is a domain expert that must evaluate the answer generated by an AI system of your questions. \n",
    "Your role is to generate a dataset of questions to test a movie expert AI system. Note that the questions could be vary and follow \n",
    "\n",
    "RULES:\n",
    "- The questions should test ONLY the following product features: {features}.\n",
    "- The questions should test ONLY the following usage scenarios: {scenarios}\n",
    "- You must generate the questions impersonating ONLY the following personas: {personas}\n",
    "\"\"\"\n",
    "\n",
    "MAX_ROWS = 20\n",
    "EVAL_CONSTRUCTION_PROMPT = (\n",
    "    \"\"\"Generate an evaluation dataset with no more than {n_rows} rows\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_dataset_builder_system_message(\n",
    "    system_message_format: str,\n",
    "    features: list[str],\n",
    "    scenarios: list[str],\n",
    "    personas: list[str],\n",
    ") -> str:\n",
    "    features = \", \".join(features)\n",
    "    scenarios = \", \".join(scenarios)\n",
    "    personas = \", \".join(personas)\n",
    "\n",
    "    return SYSTEM_MESSAGE.format(\n",
    "        features=features, scenarios=scenarios, personas=personas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = build_eval_dataset_builder_system_message(\n",
    "    SYSTEM_MESSAGE, movie_expert_features, movie_expert_scenarios, movie_expert_personas\n",
    ")\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalQuestionFormat(BaseModel):\n",
    "    question_id: int\n",
    "    question: str\n",
    "    feature: str\n",
    "    scenario: str\n",
    "    persona: str\n",
    "\n",
    "\n",
    "class EvalDataset(BaseModel):\n",
    "    questions: list[EvalQuestionFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": EVAL_CONSTRUCTION_PROMPT.format(n_rows=MAX_ROWS)},\n",
    "    ],\n",
    "    response_format=EvalDataset,\n",
    ")\n",
    "\n",
    "answer = chat_completion\n",
    "\n",
    "answer = answer.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.model_dump()[\"questions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Now, we have a list of questions to pose to our AI and ask our evaluation expert to evaluate it. Note that, the SME could be involved also both for giving you features, scenario and personas or to add particular questions to the generated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pl.from_dicts(answer.model_dump()[\"questions\"])\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers.write_csv(\"../data/eval_questions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
