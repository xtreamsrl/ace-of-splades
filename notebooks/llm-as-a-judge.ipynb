{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Let's Build an LLM-as-a-Judge Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_MESSAGE = \"\"\"You are a movie expert chatbot evaluator with advanced capabilities to understand if the question response is good or not.\n",
    "Follow some guidelines to evaluate the system:\n",
    "{guidelines}\n",
    "\n",
    "We provide you with some examples to give you an idea of how a good response should be.\n",
    "Evaluation examples:\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLE_STRUCTURE = \"\"\"<user_question>{question}</user_question>\n",
    "<response>{response}</response>\n",
    "<outcome>{outcome}</outcome>\n",
    "<critique>{critique}</critique>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationExample(BaseModel): \n",
    "    question: str\n",
    "    response: str\n",
    "    outcome: str\n",
    "    critique: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_guidelines = [\n",
    "    \"Provide a \\\"pass\\\" or \\\"fail\\\" outcome. No other category is admitted.\"\n",
    "    \"Provide a comprehensive and clear explanation of why you gave that outcome. In particular for the \\\"fail\\\" one\"\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    EvaluationExample(\n",
    "        question=\"Could you suggest an action movie?\",\n",
    "        response=\"I recommend \\\"The Thousand Faces of Dunjia\\\" (2017), directed by Yuen Woo-ping. The film follows a group of swordsmen's adventures as they secretly protect humanity by hunting down mysterious creatures from outer space. It combines action and fantasy elements, making it an engaging watch for fans of the genre. Enjoy!\",\n",
    "        outcome=\"pass\",\n",
    "        critique=\"The system replied with an action movie correctly. The response is brief but well explained.\"\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        question=\"Give me a summary of DUNE II\",\n",
    "        response=\"I'm sorry, but I don't have information about \\\"Dune II.\\\" My expertise covers movie plots, metadata, and summaries of films, but it seems that title isn't available in my current context. If you have any questions about other movies, feel free to ask!\",\n",
    "        outcome=\"fail\",\n",
    "        critique=\"The system should reply to questions regarding movie summaries. It seems that can't find the movie in the context.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evaluation_examples(example: EvaluationExample) -> str:\n",
    "    return EXAMPLE_STRUCTURE.format(question = example.question, \n",
    "                                    response = example.response, \n",
    "                                    outcome = example.outcome, \n",
    "                                    critique = example.critique)\n",
    "\n",
    "def build_evaluation_system_message(system_message: str, guidelines: str, \n",
    "                                    examples: list[EvaluationExample]) -> str:\n",
    "    formatted_guidelines = \"\\n\".join(guidelines)\n",
    "    formatted_examples = \"\\n\\n\".join([build_evaluation_examples(ex) for ex in examples])\n",
    "    return system_message.format(guidelines = formatted_guidelines, examples = formatted_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_message = build_evaluation_system_message(JUDGE_SYSTEM_MESSAGE, evaluation_guidelines, examples)\n",
    "print(formatted_system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques = pl.read_excel(\n",
    "    source=\"../data/eval_questions_with_critiques.xlsx\",\n",
    "    sheet_name=\"Sheet1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_expert_critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_questions = domain_expert_critiques.select(pl.format(\"Question: {}\\nAnswer: {}\\n\\n\", \n",
    "                                         pl.col(\"question\"), \n",
    "                                         pl.col(\"rag_answer\")\n",
    "                                        )\n",
    "                              ).to_series(0).str.join(\"\\n\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Evaluate how our AI system answered the given questions.\n",
    "Here is the list of question and answer couples: \n",
    "{formatted_questions}\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudgeDataset(BaseModel):\n",
    "    critiques: list[EvaluationExample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": formatted_system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=LLMJudgeDataset\n",
    ")\n",
    "\n",
    "llm_judge_outcome = chat_completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_df = pl.from_dicts(llm_judge_outcome.model_dump()['critiques'])\n",
    "llm_judge_df = llm_judge_df.rename({\"outcome\": \"Judgement_llm_judge\", \"critique\": \"Critique_llm_judge\"})\n",
    "llm_judge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_judges = pl.concat([domain_expert_critiques, \n",
    "                         llm_judge_df.select([\"Judgement_llm_judge\", \"Critique_llm_judge\"])], \n",
    "                        how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_judges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
